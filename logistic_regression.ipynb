{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Logistic Regression:Theory Questions and Answers\n",
        "---\n",
        "\n",
        "## \\*\\*Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "**Logistic Regression** is a supervised machine learning algorithm used for **classification problems**, especially **binary classification**. It estimates the **probability** that a data point belongs to a particular class (typically class 1).\n",
        "\n",
        "### 🔄 Difference from Linear Regression:\n",
        "\n",
        "| Feature           | Logistic Regression                 | Linear Regression              |\n",
        "| ----------------- | ----------------------------------- | ------------------------------ |\n",
        "| Purpose           | Classification                      | Regression (continuous values) |\n",
        "| Output Range      | 0 to 1 (interpreted as probability) | Any real number                |\n",
        "| Function Used     | Sigmoid Function                    | Linear Function                |\n",
        "| Output Type       | Class Label (after thresholding)    | Numeric value                  |\n",
        "| Decision Boundary | Yes (based on threshold)            | No decision boundary           |\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 2: What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "The logistic regression model predicts the probability using the **sigmoid function** applied to a linear combination of input features:\n",
        "\n",
        "$$\n",
        "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $h_\\theta(x)$: Predicted probability that the output $y = 1$\n",
        "* $\\theta$: Weight (parameter) vector\n",
        "* $x$: Input feature vector\n",
        "* $\\theta^T x$: Dot product of $\\theta$ and $x$\n",
        "* $e$: Euler’s number (\\~2.718)\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 3: Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "The **Sigmoid function** converts any real-valued number into a value **between 0 and 1**, which can be interpreted as a **probability**.\n",
        "\n",
        "### Sigmoid Function Formula:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "### ✅ Benefits:\n",
        "\n",
        "* Ensures output is between 0 and 1.\n",
        "* Allows probabilistic interpretation.\n",
        "* Smooth and differentiable (good for optimization).\n",
        "* Monotonically increasing (greater inputs yield greater probabilities).\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 4: What is the cost function of Logistic Regression?**\n",
        "\n",
        "Logistic Regression uses the **Binary Cross-Entropy Loss** (also called **Log Loss**) as its cost function:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $m$: Number of training examples\n",
        "* $y^{(i)}$: Actual class label (0 or 1)\n",
        "* $h_\\theta(x^{(i)})$: Predicted probability\n",
        "\n",
        "This cost function penalizes predictions that are far from the actual label.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 5: What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "**Regularization** is a technique to prevent **overfitting** by discouraging overly complex models with large weights. It adds a **penalty term** to the cost function.\n",
        "\n",
        "### Two Common Types:\n",
        "\n",
        "* **L1 Regularization (Lasso):** Adds absolute values of weights\n",
        "* **L2 Regularization (Ridge):** Adds squared values of weights\n",
        "\n",
        "### Modified Cost Function with L2:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Log Loss} + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\lambda$: Regularization strength (hyperparameter)\n",
        "* $m$: Number of examples\n",
        "\n",
        "### ✅ Purpose of Regularization:\n",
        "\n",
        "* Controls model complexity\n",
        "* Prevents overfitting\n",
        "* Encourages smaller, more generalizable weights\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "luyoYn47a8aH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 6: Explain the difference between Lasso, Ridge, and Elastic Net Regression**\n",
        "\n",
        "These are all **regularization techniques** used to prevent overfitting by adding a penalty to the loss function.\n",
        "\n",
        "| Type            | Regularization Term       | Effect on Weights             | Feature Selection               |                             |     |\n",
        "| --------------- | ------------------------- | ----------------------------- | ------------------------------- | --------------------------- | --- |\n",
        "| **Lasso**       | ( \\lambda \\sum            | \\theta\\_j                     | )                               | Can shrink some to **zero** | Yes |\n",
        "| **Ridge**       | $\\lambda \\sum \\theta_j^2$ | Shrinks but **does not zero** | No                              |                             |     |\n",
        "| **Elastic Net** | ( \\lambda\\_1 \\sum         | \\theta\\_j                     | + \\lambda\\_2 \\sum \\theta\\_j^2 ) | Combines both               | Yes |\n",
        "\n",
        "### 📌 Summary:\n",
        "\n",
        "* **Lasso** is good for **sparse models** (feature selection).\n",
        "* **Ridge** is good when **all features are useful** but need to be shrunk.\n",
        "* **Elastic Net** is a hybrid that performs well when features are **correlated**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 7: When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "Use **Elastic Net** when:\n",
        "\n",
        "* There are **highly correlated features**.\n",
        "* You want **feature selection** (like Lasso) but also **stability** (like Ridge).\n",
        "* The number of predictors $p$ is **greater than** the number of observations $n$.\n",
        "* Lasso selects **too few features** or Ridge does **not perform feature elimination**.\n",
        "\n",
        "### ✅ Key Benefit:\n",
        "\n",
        "Elastic Net avoids the **limitations** of Lasso and Ridge by combining both regularization effects.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 8: What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "\n",
        "The regularization parameter $\\lambda$ (also written as `C = 1/λ` in scikit-learn) **controls the strength** of the penalty.\n",
        "\n",
        "| λ Value        | Effect on Model                      |\n",
        "| -------------- | ------------------------------------ |\n",
        "| **λ → 0**      | Less regularization → overfitting    |\n",
        "| **λ → ∞**      | Strong regularization → underfitting |\n",
        "| **Moderate λ** | Best generalization                  |\n",
        "\n",
        "### ✅ Interpretation:\n",
        "\n",
        "* **Small λ** allows weights to grow → can overfit.\n",
        "* **Large λ** shrinks weights → simpler model → might underfit.\n",
        "* **Tuning λ** helps find the **right bias-variance trade-off**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 9: What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "While logistic regression is **less restrictive** than linear regression, it still makes several important assumptions:\n",
        "\n",
        "1. **Binary output variable** (for basic logistic regression).\n",
        "2. **Linearity of log-odds**: The logit (log-odds) of the outcome is a **linear combination** of input features.\n",
        "3. **Independence of observations**.\n",
        "4. **Low multicollinearity**: Predictors should not be highly correlated.\n",
        "5. **Large sample size**: Especially when features are many.\n",
        "6. **No outliers**: Outliers can bias the model due to the log-likelihood optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 10: What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "When logistic regression isn't sufficient (e.g., for non-linear problems or complex datasets), you can use:\n",
        "\n",
        "### ✅ Common Alternatives:\n",
        "\n",
        "1. **Decision Trees**\n",
        "2. **Random Forests**\n",
        "3. **Gradient Boosting (e.g., XGBoost, LightGBM)**\n",
        "4. **Support Vector Machines (SVM)**\n",
        "5. **k-Nearest Neighbors (KNN)**\n",
        "6. **Naive Bayes**\n",
        "7. **Neural Networks**\n",
        "8. **Quadratic Discriminant Analysis (QDA)**\n",
        "\n",
        "### 🔍 When to switch?\n",
        "\n",
        "* Logistic Regression struggles with **non-linear** decision boundaries.\n",
        "* Alternatives may handle **interactions**, **non-linearity**, and **complex patterns** better.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CI8ku1YHbF4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Question 11: What are Classification Evaluation Metrics?**\n",
        "\n",
        "Classification metrics evaluate how well a model distinguishes between classes.\n",
        "\n",
        "### ✅ Common Metrics:\n",
        "\n",
        "1. **Accuracy**\n",
        "\n",
        "   $$\n",
        "   \\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}\n",
        "   $$\n",
        "\n",
        "   Proportion of correctly predicted labels.\n",
        "\n",
        "2. **Precision**\n",
        "\n",
        "   $$\n",
        "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
        "   $$\n",
        "\n",
        "   Correctness among predicted positives.\n",
        "\n",
        "3. **Recall (Sensitivity or TPR)**\n",
        "\n",
        "   $$\n",
        "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\n",
        "   $$\n",
        "\n",
        "   Ability to find all actual positives.\n",
        "\n",
        "4. **F1-Score**\n",
        "\n",
        "   $$\n",
        "   \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
        "   $$\n",
        "\n",
        "   Harmonic mean of precision and recall.\n",
        "\n",
        "5. **ROC-AUC**\n",
        "\n",
        "   * Measures area under the **Receiver Operating Characteristic** curve.\n",
        "   * High AUC means good model separation.\n",
        "\n",
        "6. **Confusion Matrix**\n",
        "   A table showing TP, TN, FP, FN to visualize model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 12: How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "### ❌ Problem:\n",
        "\n",
        "When one class is much more frequent than the other (e.g., 95% vs 5%), logistic regression **leans toward predicting the majority class**, leading to:\n",
        "\n",
        "* **High accuracy** but **low recall** for the minority class.\n",
        "* Poor **F1-score** and misleading performance.\n",
        "\n",
        "### ✅ Solutions:\n",
        "\n",
        "1. **Class Weights**\n",
        "   Use `class_weight='balanced'` in scikit-learn to give more importance to the minority class.\n",
        "\n",
        "2. **Oversampling**\n",
        "   Duplicate minority class examples (e.g., SMOTE).\n",
        "\n",
        "3. **Undersampling**\n",
        "   Reduce majority class examples.\n",
        "\n",
        "4. **Evaluation Metrics**\n",
        "   Focus on **precision**, **recall**, and **AUC**, not just accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 13: What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "Hyperparameter tuning is the process of selecting the **best model parameters** that aren’t learned directly from the data.\n",
        "\n",
        "### 🔧 Important Hyperparameters:\n",
        "\n",
        "1. **C (Inverse of λ):** Controls regularization strength.\n",
        "2. **Penalty:** Choose between `'l1'`, `'l2'`, or `'elasticnet'`.\n",
        "3. **Solver:** Optimization algorithm (`'liblinear'`, `'saga'`, `'lbfgs'`).\n",
        "4. **Class Weight:** `'balanced'` helps with imbalanced data.\n",
        "5. **Max Iterations:** Controls convergence speed.\n",
        "\n",
        "### ✅ Methods for Tuning:\n",
        "\n",
        "* **Grid Search**\n",
        "* **Random Search**\n",
        "* **Bayesian Optimization**\n",
        "* **Cross-Validation** (usually k-fold)\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 14: What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "Solvers are optimization algorithms used to **minimize the cost function** in logistic regression.\n",
        "\n",
        "| Solver        | Supports           | Use Case                                 |\n",
        "| ------------- | ------------------ | ---------------------------------------- |\n",
        "| **liblinear** | L1, L2             | Good for small datasets, sparse data     |\n",
        "| **lbfgs**     | L2                 | Default for large datasets               |\n",
        "| **saga**      | L1, L2, ElasticNet | Fast, handles large-scale problems       |\n",
        "| **newton-cg** | L2                 | Second-order method, supports multiclass |\n",
        "\n",
        "### ✅ Recommendations:\n",
        "\n",
        "* Use **`lbfgs`** for general-purpose and multiclass problems.\n",
        "* Use **`saga`** for large-scale or elastic net regularization.\n",
        "* Use **`liblinear`** for binary classification with L1 penalty.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 15: How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "Logistic Regression handles multiclass classification using:\n",
        "\n",
        "### 1. **One-vs-Rest (OvR)**:\n",
        "\n",
        "* Trains **one classifier per class** vs all others.\n",
        "* Final class = highest probability output.\n",
        "* Supported by most libraries (default in scikit-learn).\n",
        "\n",
        "### 2. **Multinomial (Softmax Regression)**:\n",
        "\n",
        "* Directly models **probability distribution** over multiple classes.\n",
        "* Uses **softmax function** instead of sigmoid.\n",
        "\n",
        "$$\n",
        "P(y = k \\mid x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\theta_k$: Weights for class $k$\n",
        "* $K$: Total number of classes\n",
        "\n",
        "### ✅ Use Cases:\n",
        "\n",
        "* **OvR**: Simple, good for **binary-like** multiclass setups.\n",
        "* **Softmax**: Better for **balanced and correlated** multiclass problems.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uwgBhRaQbPQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 16: What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "### ✅ Advantages:\n",
        "\n",
        "1. **Simple & Interpretable**\n",
        "   Easy to understand and explain results.\n",
        "\n",
        "2. **Fast Training**\n",
        "   Low computational cost, even on large datasets.\n",
        "\n",
        "3. **Probabilistic Output**\n",
        "   Predicts class probabilities, useful for decision thresholds.\n",
        "\n",
        "4. **Works Well with Linearly Separable Data**\n",
        "\n",
        "5. **Regularization Support**\n",
        "   Can include L1, L2 to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ Disadvantages:\n",
        "\n",
        "1. **Assumes Linearity of Log-Odds**\n",
        "   Cannot model non-linear decision boundaries unless features are transformed.\n",
        "\n",
        "2. **Poor with Outliers**\n",
        "   Sensitive to extreme values.\n",
        "\n",
        "3. **Limited for Complex Relationships**\n",
        "   Underperforms when interactions or non-linearities are important.\n",
        "\n",
        "4. **Not Ideal for Multicollinearity**\n",
        "   High correlation between variables can distort results.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 17: What are some use cases of Logistic Regression?**\n",
        "\n",
        "Logistic Regression is widely used across industries for binary or multiclass classification tasks.\n",
        "\n",
        "### ✅ Common Use Cases:\n",
        "\n",
        "1. **Medical Diagnosis**\n",
        "   Predict if a patient has a disease (yes/no).\n",
        "\n",
        "2. **Marketing**\n",
        "   Will a user click an ad or not?\n",
        "\n",
        "3. **Finance**\n",
        "   Credit scoring – will a user default on a loan?\n",
        "\n",
        "4. **Spam Detection**\n",
        "   Email classification (spam or not).\n",
        "\n",
        "5. **Fraud Detection**\n",
        "   Is this transaction fraudulent?\n",
        "\n",
        "6. **Churn Prediction**\n",
        "   Will a customer leave the service?\n",
        "\n",
        "7. **Social Science Research**\n",
        "   Predict likelihood of voting, employment status, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 18: What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "| Feature             | Logistic Regression                | Softmax Regression (Multinomial LR)                    |\n",
        "| ------------------- | ---------------------------------- | ------------------------------------------------------ |\n",
        "| Output Classes      | Two (binary)                       | More than two (multiclass)                             |\n",
        "| Activation Function | Sigmoid                            | Softmax                                                |\n",
        "| Equation            | $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ | $\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ |\n",
        "| Use Case            | Binary classification              | Multiclass classification                              |\n",
        "\n",
        "### ✅ Summary:\n",
        "\n",
        "* **Logistic Regression**: Uses **sigmoid** for binary problems.\n",
        "* **Softmax Regression**: Uses **softmax** for multiclass problems (predicts probabilities for **all classes** simultaneously).\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 19: How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "### ✅ Use **OvR (One-vs-Rest)** when:\n",
        "\n",
        "* Classes are **well-separated**.\n",
        "* You want **interpretability**.\n",
        "* Dataset is **small or sparse**.\n",
        "* Using a library that doesn’t support multinomial natively.\n",
        "\n",
        "### ✅ Use **Softmax (Multinomial)** when:\n",
        "\n",
        "* Classes are **mutually exclusive** and **interdependent**.\n",
        "* You care about **true class probabilities**.\n",
        "* Dataset is **large**, well-distributed, and classes are **correlated**.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Key Difference:\n",
        "\n",
        "* **OvR** builds **K binary classifiers** for $K$ classes.\n",
        "* **Softmax** builds **one model** that learns **all classes at once**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 20: How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "Each coefficient $\\beta_j$ shows how the **log-odds** of the outcome change with a one-unit increase in feature $x_j$.\n",
        "\n",
        "$$\n",
        "\\text{Logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "### ✅ Interpretations:\n",
        "\n",
        "1. **Positive Coefficient**:\n",
        "   As $x_j$ increases, the **odds** of the positive class increase.\n",
        "\n",
        "2. **Negative Coefficient**:\n",
        "   As $x_j$ increases, the **odds** of the positive class decrease.\n",
        "\n",
        "3. **Exponentiated Coefficient** $e^{\\beta_j}$:\n",
        "   Represents **multiplicative change** in the odds.\n",
        "\n",
        "### 🔍 Example:\n",
        "\n",
        "If $\\beta_2 = 0.7$, then:\n",
        "\n",
        "$$\n",
        "e^{0.7} \\approx 2.01\n",
        "$$\n",
        "\n",
        "This means the **odds double** for a one-unit increase in $x_2$, holding other variables constant.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oMyoDh7nbVLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###practical questions"
      ],
      "metadata": {
        "id": "Y_CYmLaRcTMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlwqK4lDa7Ev",
        "outputId": "fd01dc0f-f112-42c8-dcfa-116d579638d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ],
      "source": [
        "# QUESTION 1: Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 2: Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1-Regularized Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znF84dmicYAP",
        "outputId": "0366d52e-4126-40fa-d32d-5b466df047ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 3: Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L2-Regularized Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcwtottMcY10",
        "outputId": "c1b422f6-193b-4e36-edd2-1b16d8b067e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Accuracy: 0.956140350877193\n",
            "Coefficients: [[ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n",
            "  -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n",
            "  -2.07613380e-01 -2.97739324e-02 -5.00338038e-02  1.44298427e+00\n",
            "  -3.03857384e-01 -7.25692126e-02 -1.61591524e-02 -1.90655332e-03\n",
            "  -4.48855442e-02 -3.77188737e-02 -4.17516190e-02  5.61347410e-03\n",
            "   1.23214996e+00 -4.04581097e-01 -3.62091502e-02 -2.70867580e-02\n",
            "  -2.62630530e-01 -1.20898539e+00 -1.61796947e+00 -6.15250835e-01\n",
            "  -7.42763610e-01 -1.16960181e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 4: Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG_eAYHTcbzA",
        "outputId": "441f700f-4b37-4623-c51d-08e11f8e1d02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 5: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Multiclass OvR Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y1Jlc33chBs",
        "outputId": "12e19e33-6413-405c-d5c5-33fafb0e55b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass OvR Accuracy: 0.9611111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 6: Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3sb0Wywci-5",
        "outputId": "dbcdba8a-b48c-4496-ce0e-5ee12d2fd8a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 6: Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00jSgQY4ck82",
        "outputId": "d48b47d2-979a-4751-8c2c-83ecbe19182f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 7: Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "print(\"Stratified K-Fold Accuracies:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBq1xOYncm8_",
        "outputId": "0f1f22df-5cf7-4fcc-b2eb-7933a40122b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified K-Fold Accuracies: [0.96491228 0.92105263 0.96491228 0.94736842 0.97345133]\n",
            "Average Accuracy: 0.9543393882937432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 8: Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "# STEP 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# STEP 2: Download the Iris dataset directly from GitHub\n",
        "url = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# STEP 3: Split data into features and target\n",
        "X = df.drop('species', axis=1)\n",
        "y = df['species']\n",
        "\n",
        "# STEP 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# STEP 5: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# STEP 6: Make predictions and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy on Iris Dataset:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Jwlacocpdk",
        "outputId": "238bd532-a8ec-4948-8595-755fc69cdfff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy on Iris Dataset: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 9: Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "rand_search = RandomizedSearchCV(LogisticRegression(max_iter=10000), param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
        "rand_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", rand_search.best_params_)\n",
        "print(\"Best Accuracy:\", rand_search.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqbLO-19cskb",
        "outputId": "68c4669e-2edd-4dd3-a02a-d07c53b889fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(7.3299394181140505), 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 10: Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=10000))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "print(\"One-vs-One Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRDW2HicdDJy",
        "outputId": "f4c165d2-31d1-4f47-8f20-021c0717d261"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Accuracy: 0.9833333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 11: Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "QIwILSQwdKsg",
        "outputId": "09ff8592-fcad-4f55-d0bd-de59df90d18a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 12: Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZTWFLbmeazh",
        "outputId": "4faae3aa-a021-4248-8d5f-69ac4fe68e38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9459459459459459\n",
            "Recall: 0.9859154929577465\n",
            "F1-Score: 0.9655172413793104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 13: Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuvU1HuaefXD",
        "outputId": "bba5f8bb-cf22-4ea6-a7b4-26d7aa37f0bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.92       180\n",
            "           1       0.41      0.75      0.53        20\n",
            "\n",
            "    accuracy                           0.86       200\n",
            "   macro avg       0.69      0.81      0.72       200\n",
            "weighted avg       0.91      0.86      0.88       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 14: Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values without chained assignment warnings\n",
        "df.fillna({'Age': df['Age'].median(), 'Embarked': df['Embarked'].mode()[0]}, inplace=True)\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Feature selection\n",
        "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Titanic Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quneP9NGeiz1",
        "outputId": "792a4416-f65d-45d8-bda7-e68c7fe70dcf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic Accuracy: 0.7821229050279329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 15: Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "model1 = LogisticRegression(max_iter=10000)\n",
        "model1.fit(X_train, y_train)\n",
        "y_pred1 = model1.predict(X_test)\n",
        "acc1 = accuracy_score(y_test, y_pred1)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model2 = LogisticRegression(max_iter=10000)\n",
        "model2.fit(X_train_scaled, y_train)\n",
        "y_pred2 = model2.predict(X_test_scaled)\n",
        "acc2 = accuracy_score(y_test, y_pred2)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc1)\n",
        "print(\"Accuracy with scaling:\", acc2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gSTY6Q4ffD-",
        "outputId": "3c7865b9-a601-4158-dce6-bd3934493a20"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 16: Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYRFsZvqemjB",
        "outputId": "1158dd0d-d483-428b-a9d7-c1177eeddc07"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977071732721913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 17: Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy with C=0.5:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vKtLBjFfmBr",
        "outputId": "23cbc984-6be9-492b-f1db-cab032865fa3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 18: Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "feature_names = load_breast_cancer().feature_names\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X, y)\n",
        "\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDzdHTNGfpYy",
        "outputId": "1e7f5bc2-1d3e-4aa7-aa9f-4caf3f4af983"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Feature  Coefficient\n",
            "11            texture error     1.269702\n",
            "0               mean radius     0.998294\n",
            "1              mean texture     0.182224\n",
            "20             worst radius     0.141487\n",
            "12          perimeter error     0.123329\n",
            "15        compactness error     0.077448\n",
            "3                 mean area     0.022930\n",
            "19  fractal dimension error     0.015566\n",
            "23               worst area    -0.013620\n",
            "14         smoothness error    -0.026667\n",
            "9    mean fractal dimension    -0.029688\n",
            "16          concavity error    -0.030446\n",
            "18           symmetry error    -0.034343\n",
            "17     concave points error    -0.039286\n",
            "10             radius error    -0.079991\n",
            "29  worst fractal dimension    -0.092212\n",
            "22          worst perimeter    -0.106278\n",
            "13               area error    -0.109478\n",
            "4           mean smoothness    -0.183688\n",
            "5          mean compactness    -0.214135\n",
            "8             mean symmetry    -0.271601\n",
            "2            mean perimeter    -0.277385\n",
            "7       mean concave points    -0.303780\n",
            "24         worst smoothness    -0.369395\n",
            "21            worst texture    -0.439015\n",
            "6            mean concavity    -0.541417\n",
            "27     worst concave points    -0.621370\n",
            "25        worst compactness    -0.665711\n",
            "28           worst symmetry    -0.733936\n",
            "26          worst concavity    -1.438114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 19: Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", kappa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GIXgD9YfrZG",
        "outputId": "6ad4d054-5722-4fe1-efb7-bef1b51789e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9053470607771504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 20: Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "QmQ9AVYIftrS",
        "outputId": "f9174f77-5991-4af1-ae95-9831f86f39f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 21: Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=10000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with solver={solver}: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb_GgrT6fv46",
        "outputId": "497b00bf-90ca-44f8-b236-1e863e8605aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver=liblinear: 0.956140350877193\n",
            "Accuracy with solver=saga: 0.9736842105263158\n",
            "Accuracy with solver=lbfgs: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 22: Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient:\", mcc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZszSkJpBf1vC",
        "outputId": "ffa43a1f-fd77-45f8-935f-ac942d08f85e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.9068106119605033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 23: Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Raw data\n",
        "model_raw = LogisticRegression(max_iter=10000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy on raw data:\", acc_raw)\n",
        "print(\"Accuracy on standardized data:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10xIL_gIf4eq",
        "outputId": "b62b5738-3d46-42f4-9edc-7f71b7309586"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.956140350877193\n",
            "Accuracy on standardized data: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 24: Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best C:\", grid.best_params_['C'])\n",
        "print(\"Best cross-validation accuracy:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "print(\"Test set accuracy:\", best_model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHeKekjSf8Du",
        "outputId": "30d59d82-b24b-4f1e-803f-fe4cdc969a8e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 100\n",
            "Best cross-validation accuracy: 0.9692307692307693\n",
            "Test set accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 25: Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "\n",
        "# Load model\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "\n",
        "# Predict\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(\"Loaded model accuracy:\", loaded_model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhNDONPZf6Al",
        "outputId": "33d28ccd-4843-4159-d8e8-e62d918c3be4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model accuracy: 0.956140350877193\n"
          ]
        }
      ]
    }
  ]
}